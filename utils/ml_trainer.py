import os
import pickle
import argparse
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# Machine learning imports
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer, f1_score
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectFromModel
from sklearn.inspection import permutation_importance

# For advanced models if needed
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False


class MLRerouteTrainer:
    """
    Trains a machine learning model to predict the optimal reroute action
    based on the training data generated by MLDataGenerator.
    """
    
    MODEL_DIR = os.path.join('models', 'resolvers', 'saved_models')
    DEFAULT_MODEL_PATH = os.path.join(MODEL_DIR, 'reroute_classifier.pkl')
    
    def __init__(self, training_data_path: str, model_type: str = 'random_forest'):
        self.training_data_path = training_data_path
        self.model_type = model_type
        
        # Create directory if it doesn't exist
        os.makedirs(self.MODEL_DIR, exist_ok=True)
        
        self.feature_cols = None
        self.label_col = 'best_action'
        
        # Metrics storage
        self.train_metrics = {}
        self.test_metrics = {}
        self.cv_metrics = {}
        self.feature_importance = {}
    
    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        Load and split the training data
        
        Returns:
            X_train, X_test, y_train, y_test
        """
        print(f"Loading training data from {self.training_data_path}")
        try:
            data = pd.read_csv(self.training_data_path)
            print(f"Loaded {len(data)} samples")
            
            # Check if this is a full or simple dataset
            if 'disruption_id' in data.columns:
                # Full dataset, exclude non-feature columns
                exclude_cols = ['best_action', 'disruption_id', 'driver_id']
                
                # Check and handle dictionary columns
                dict_cols = []
                for col in data.columns:
                    if col.startswith('all_actions_'):
                        exclude_cols.append(col)
                
                self.feature_cols = [col for col in data.columns if col not in exclude_cols]
            else:
                # Simple dataset, all columns except the label are features
                self.feature_cols = [col for col in data.columns if col != self.label_col]
            
            print(f"Using {len(self.feature_cols)} features: {self.feature_cols}")
            
            # Split data
            X = data[self.feature_cols]
            y = data[self.label_col]
            
            # Train-test split with stratification
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            print(f"Split into {len(X_train)} training and {len(X_test)} testing samples")
            
            # Check class balance
            class_counts = y.value_counts(normalize=True) * 100
            print("\nClass distribution in data:")
            for cls, pct in class_counts.items():
                print(f"{cls}: {pct:.2f}%")
            
            return X_train, X_test, y_train, y_test
            
        except Exception as e:
            print(f"Error loading data: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def train_model(self, output_path: Optional[str] = None) -> object:
        """
        Train the model on the loaded data with cross-validation and feature selection
        
        Args:
            output_path: Path to save the trained model to, defaults to class default
            
        Returns:
            Trained model
        """
        try:
            # Load and split data
            X_train, X_test, y_train, y_test = self.load_data()
            
            # Create base model for feature selection
            base_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,  # Limit depth to prevent overfitting
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                class_weight='balanced'
            )
            
            # Perform feature selection
            print("\nPerforming feature selection...")
            selector = SelectFromModel(base_model, prefit=False)
            selector.fit(X_train, y_train)
            
            # Get selected features
            selected_features = X_train.columns[selector.get_support()].tolist()
            print(f"Selected {len(selected_features)} features: {selected_features}")
            
            # Update feature columns
            X_train_selected = X_train[selected_features]
            X_test_selected = X_test[selected_features]
            
            # Create final model with selected features
            model = self._create_model()
            
            # Perform cross-validation
            print("\nPerforming cross-validation...")
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            cv_scores = cross_val_score(
                model, X_train_selected, y_train, 
                cv=cv, scoring='f1_weighted', n_jobs=-1
            )
            
            self.cv_metrics = {
                'mean_score': cv_scores.mean(),
                'std_score': cv_scores.std(),
                'scores': cv_scores
            }
            
            print(f"Cross-validation scores: {cv_scores}")
            print(f"Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            
            # Train final model
            print(f"\nTraining final {self.model_type} model...")
            model.fit(X_train_selected, y_train)
            
            # Calculate feature importance
            if hasattr(model, 'named_steps') and 'classifier' in model.named_steps:
                classifier = model.named_steps['classifier']
                if hasattr(classifier, 'feature_importances_'):
                    self.feature_importance = dict(zip(selected_features, classifier.feature_importances_))
            
            # Evaluate on training data
            y_train_pred = model.predict(X_train_selected)
            train_accuracy = accuracy_score(y_train, y_train_pred)
            train_report = classification_report(y_train, y_train_pred, output_dict=True)
            
            # Evaluate on testing data
            y_test_pred = model.predict(X_test_selected)
            test_accuracy = accuracy_score(y_test, y_test_pred)
            test_report = classification_report(y_test, y_test_pred, output_dict=True)
            
            # Store metrics
            self.train_metrics = {
                'accuracy': train_accuracy,
                'report': train_report
            }
            
            self.test_metrics = {
                'accuracy': test_accuracy,
                'report': test_report,
                'confusion_matrix': confusion_matrix(y_test, y_test_pred)
            }
            
            # Print results
            print(f"\nTraining accuracy: {train_accuracy:.4f}")
            print(f"Testing accuracy: {test_accuracy:.4f}")
            print("\nClassification report on test data:")
            print(classification_report(y_test, y_test_pred))
            
            # Save model
            if output_path is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_path = os.path.join(self.MODEL_DIR, f"reroute_classifier_{timestamp}.pkl")
            
            # Save model and copy to default path
            with open(output_path, 'wb') as f:
                pickle.dump(model, f)
            
            # Also save to default path for the application to use
            with open(self.DEFAULT_MODEL_PATH, 'wb') as f:
                pickle.dump(model, f)
            
            print(f"\nModel saved to {output_path}")
            print(f"Model also saved to default path: {self.DEFAULT_MODEL_PATH}")
            
            # Save metadata with training performance
            metadata_path = os.path.join(self.MODEL_DIR, f"model_metadata_{timestamp}.txt")
            with open(metadata_path, 'w') as f:
                f.write(f"Model type: {self.model_type}\n")
                f.write(f"Training data: {self.training_data_path}\n")
                f.write(f"Selected features: {selected_features}\n")
                f.write(f"Training samples: {len(X_train)}\n")
                f.write(f"Testing samples: {len(X_test)}\n")
                f.write(f"Cross-validation mean score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\n")
                f.write(f"Training accuracy: {train_accuracy:.4f}\n")
                f.write(f"Testing accuracy: {test_accuracy:.4f}\n\n")
                f.write("Feature importance:\n")
                for feature, importance in self.feature_importance.items():
                    f.write(f"{feature}: {importance:.4f}\n")
                f.write("\nClassification report on test data:\n")
                f.write(str(classification_report(y_test, y_test_pred)))
            
            print(f"Model metadata saved to {metadata_path}")
            
            return model
            
        except Exception as e:
            print(f"Error training model: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _create_model(self) -> object:
        """
        Create the machine learning model based on the specified type
        """
        if self.model_type == 'random_forest':
            # Create a pipeline with preprocessing and the model
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,  # Limit depth to prevent overfitting
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    class_weight='balanced'
                ))
            ])
            return pipeline
            
        elif self.model_type == 'random_forest_tuned':
            # Create a pipeline with preprocessing and the model with hyperparameter tuning
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', RandomForestClassifier(random_state=42))
            ])
            
            param_grid = {
                'classifier__n_estimators': [50, 100, 200],
                'classifier__max_depth': [5, 10, 15, 20],
                'classifier__min_samples_split': [2, 5, 10],
                'classifier__min_samples_leaf': [1, 2, 4],
                'classifier__class_weight': ['balanced', None]
            }
            
            grid_search = GridSearchCV(
                pipeline, param_grid, cv=5, 
                scoring='f1_weighted',  # Use F1 score for imbalanced classes
                n_jobs=-1
            )
            
            return grid_search
            
        elif self.model_type == 'xgboost' and HAS_XGB:
            # Create XGBoost model if available
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', xgb.XGBClassifier(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=5,
                    min_child_weight=2,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    random_state=42
                ))
            ])
            return pipeline
            
        else:
            # Default to random forest if the specified type is unavailable
            print(f"Model type '{self.model_type}' not recognized or required library not installed.")
            print("Defaulting to random forest classifier.")
            
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    class_weight='balanced'
                ))
            ])
            return pipeline


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train a machine learning model for reroute action prediction')
    parser.add_argument('--data', type=str, required=True, help='Path to the training data CSV file')
    parser.add_argument('--model', type=str, default='random_forest', 
                        choices=['random_forest', 'random_forest_tuned', 'xgboost'],
                        help='Type of model to train')
    parser.add_argument('--output', type=str, default=None, 
                        help='Path to save the trained model (default: auto-generated)')
    
    args = parser.parse_args()
    
    trainer = MLRerouteTrainer(
        training_data_path=args.data,
        model_type=args.model
    )
    
    trainer.train_model(output_path=args.output) 